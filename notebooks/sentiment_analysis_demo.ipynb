{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Sentiment Classification Demo\n",
    "\n",
    "This notebook demonstrates the text sentiment classification model that predicts whether a given text expresses a positive or negative sentiment. The model uses TF-IDF features with a simple machine learning classifier for fast training and easy deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Add the src directory to the path so we can import our modules\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "# Import our modules\n",
    "from data_prep import download_movie_reviews, preprocess_data, split_and_save_data\n",
    "from feature_extract import TfidfFeatureExtractor\n",
    "from train import train_model\n",
    "from evaluate import evaluate_model, plot_confusion_matrix, plot_roc_curve\n",
    "from predict import predict_sentiment, preprocess_text\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Let's download and prepare the dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "df = download_movie_reviews()\n",
    "\n",
    "# Display the first few rows\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check class distribution\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(f'Class distribution:\n{sentiment_counts}')\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', data=df, palette=['#ff9999', '#66b3ff'])\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Sentiment (0=Negative, 1=Positive)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Display a few examples after preprocessing\n",
    "print('Examples after preprocessing:')\n",
    "for i, row in df.sample(5).iterrows():\n",
    "    print(f\"\\nSentiment: {'Positive' if row['sentiment'] == 1 else 'Negative'}\")\n",
    "    print(f\"Text: {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = split_and_save_data(df)\n",
    "\n",
    "print(f'Training set shape: {train_df.shape}')\n",
    "print(f'Testing set shape: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Now, let's extract TF-IDF features from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the TF-IDF feature extractor\n",
    "extractor = TfidfFeatureExtractor(max_features=5000, min_df=5, max_df=0.7, ngram_range=(1, 2))\n",
    "\n",
    "# Extract features\n",
    "X_train = extractor.fit_transform(train_df['text'])\n",
    "X_test = extractor.transform(test_df['text'])\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['sentiment']\n",
    "y_test = test_df['sentiment']\n",
    "\n",
    "print(f'Training features shape: {X_train.shape}')\n",
    "print(f'Testing features shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get the top features (terms) by TF-IDF score\n",
    "feature_names = extractor.get_feature_names()\n",
    "\n",
    "# Calculate the average TF-IDF score for each feature\n",
    "tfidf_mean = X_train.mean(axis=0).A1\n",
    "\n",
    "# Create a DataFrame of features and their scores\n",
    "feature_scores = pd.DataFrame({'feature': feature_names, 'score': tfidf_mean})\n",
    "\n",
    "# Sort by score in descending order\n",
    "top_features = feature_scores.sort_values('score', ascending=False).head(20)\n",
    "\n",
    "# Plot the top features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='score', y='feature', data=top_features, palette='viridis')\n",
    "plt.title('Top 20 Features by TF-IDF Score')\n",
    "plt.xlabel('Average TF-IDF Score')\n",
    "plt.ylabel('Feature (Term)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Let's train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the models to train\n",
    "model_types = ['logistic_regression', 'naive_bayes', 'svm']\n",
    "\n",
    "# Train and evaluate each model\n",
    "models = {}\n",
    "train_accuracies = {}\n",
    "test_metrics = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    print(f'\\nTraining {model_type} model...')\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(X_train, y_train, model_type=model_type)\n",
    "    models[model_type] = model\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_accuracies[model_type] = train_accuracy\n",
    "    print(f'Training accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    test_metrics[model_type] = {\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1': test_f1\n",
    "    }\n",
    "    \n",
    "    print(f'Test accuracy: {test_accuracy:.4f}')\n",
    "    print(f'Test precision: {test_precision:.4f}')\n",
    "    print(f'Test recall: {test_recall:.4f}')\n",
    "    print(f'Test F1 score: {test_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare model performance\n",
    "metrics_df = pd.DataFrame(test_metrics).T\n",
    "metrics_df.index.name = 'Model'\n",
    "metrics_df.reset_index(inplace=True)\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics_df_melted = pd.melt(metrics_df, id_vars=['Model'], var_name='Metric', value_name='Score')\n",
    "\n",
    "sns.barplot(x='Model', y='Score', hue='Metric', data=metrics_df_melted, palette='viridis')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the metrics table\n",
    "metrics_df.set_index('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate the best model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find the best model based on test accuracy\n",
    "best_model_type = metrics_df.loc[metrics_df['accuracy'].idxmax(), 'Model']\n",
    "best_model = models[best_model_type]\n",
    "\n",
    "print(f'Best model: {best_model_type} with accuracy {metrics_df.loc[metrics_df[\"Model\"] == best_model_type, \"accuracy\"].values[0]:.4f}')\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion Matrix - {best_model_type}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROC curve for models that support probability predictions\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_type, model in models.items():\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        try:\n",
    "            # Get probability predictions\n",
    "            y_score = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculate ROC curve and AUC\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{model_type} (AUC = {roc_auc:.2f})')\n",
    "        except:\n",
    "            print(f'Error plotting ROC curve for {model_type}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "Let's examine which features (words/terms) are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature importance for logistic regression model\n",
    "if 'logistic_regression' in models:\n",
    "    # Get the logistic regression model\n",
    "    lr_model = models['logistic_regression']\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = extractor.get_feature_names()\n",
    "    \n",
    "    # Get coefficients\n",
    "    coefficients = lr_model.coef_[0]\n",
    "    \n",
    "    # Create a DataFrame of features and their coefficients\n",
    "    feature_importance = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "    \n",
    "    # Sort by absolute coefficient value in descending order\n",
    "    feature_importance['abs_coefficient'] = np.abs(feature_importance['coefficient'])\n",
    "    feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    top_positive = feature_importance[feature_importance['coefficient'] > 0].head(15)\n",
    "    top_negative = feature_importance[feature_importance['coefficient'] < 0].head(15)\n",
    "    \n",
    "    # Plot top positive features (indicative of positive sentiment)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='coefficient', y='feature', data=top_positive, palette='Greens_r')\n",
    "    plt.title('Top 15 Features Indicating Positive Sentiment')\n",
    "    plt.xlabel('Coefficient (Importance)')\n",
    "    plt.ylabel('Feature (Term)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot top negative features (indicative of negative sentiment)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='coefficient', y='feature', data=top_negative, palette='Reds_r')\n",
    "    plt.title('Top 15 Features Indicating Negative Sentiment')\n",
    "    plt.xlabel('Coefficient (Importance)')\n",
    "    plt.ylabel('Feature (Term)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction Demo\n",
    "\n",
    "Let's use our best model to make predictions on new text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample texts for prediction\n",
    "sample_texts = [\n",
    "    \"I really enjoyed this product. It works great and the customer service was excellent!\",\n",
    "    \"This is the worst experience I've ever had. The product broke after one use.\",\n",
    "    \"The movie was okay, not great but not terrible either.\",\n",
    "    \"I'm very satisfied with my purchase and would recommend it to others.\"\n",
    "]\n",
    "\n",
    "# Make predictions using the best model\n",
    "print(f'Making predictions using the {best_model_type} model:\\n')\n",
    "\n",
    "results = []\n",
    "for text in sample_texts:\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Transform the text to TF-IDF features\n",
    "    X = extractor.transform([preprocessed_text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(X)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    probability = None\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        try:\n",
    "            probability = best_model.predict_proba(X)[0][prediction]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'text': text,\n",
    "        'sentiment': 'Positive' if prediction == 1 else 'Negative',\n",
    "        'confidence': probability\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Print prediction details\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted sentiment: {result['sentiment']}\")\n",
    "    if result['confidence'] is not None:\n",
    "        print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Create a DataFrame of results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a function for interactive prediction\n",
    "def predict_interactive(text):\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    \n",
    "    # Transform the text to TF-IDF features\n",
    "    X = extractor.transform([preprocessed_text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = best_model.predict(X)[0]\n",
    "    \n",
    "    # Get probability if available\n",
    "    probability = None\n",
    "    if hasattr(best_model, 'predict_proba'):\n",
    "        try:\n",
    "            probability = best_model.predict_proba(X)[0][prediction]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Print prediction details\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Preprocessed text: {preprocessed_text}\")\n",
    "    print(f\"Predicted sentiment: {'Positive' if prediction == 1 else 'Negative'}\")\n",
    "    if probability is not None:\n",
    "        print(f\"Confidence: {probability:.4f}\")\n",
    "    \n",
    "    return prediction, probability\n",
    "\n",
    "# Try it with your own text\n",
    "# predict_interactive(\"Your text here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated a complete text sentiment classification pipeline using TF-IDF features and machine learning classifiers. The model can effectively predict whether a given text expresses a positive or negative sentiment.\n",
    "\n",
    "Key components of the pipeline include:\n",
    "\n",
    "1. Data preparation and preprocessing\n",
    "2. TF-IDF feature extraction\n",
    "3. Model training and selection\n",
    "4. Model evaluation and analysis\n",
    "5. Prediction interface\n",
    "\n",
    "The model is simple, fast to train, and easy to deploy, making it suitable for various applications requiring sentiment analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 }
}